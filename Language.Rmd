---
title: "Languages"
output: html_document
date: "2024-05-14"
---

# Load library

```{r, echo=FALSE}

library(rvest)
library(httr)
library(readxl)
library(pdftools)
library(tidyverse)
library(cld2)
library(officer) 
library(openxlsx) 
library(stringr)
library(conflicted)
library(curl)
library(utils)
library(quanteda)
library(tm)
library(textcat)
library(tesseract)
library(fs)
library(magick)
library(readtext)
library(dplyr)
library(tools)

```


# Load data



# Loop all the files

```{r}


detect_segment_language <- function(segment) {
  if (nchar(segment) == 0) {
    return("No content")
  }
  
  result <- tryCatch({
    cld2::detect_language(segment, plain_text = TRUE)
  }, error = function(e) {
    return(NA)
  })
  
  if (is.na(result) || length(result) == 0 || result == "UNKNOWN") {
    return("Uncertain")
  } else {
    return(result)
  }
}

process_pdf <- function(file_path) {
  text_from_pdf <- tryCatch({
    pdf_text(file_path)
  }, error = function(e) {
    return(NULL)
  })
  
  if (is.null(text_from_pdf)) {
    return(NULL)
  }
  
  combined_text <- paste(text_from_pdf, collapse = " ")
  text_segments <- unlist(strsplit(combined_text, "[\\n\\.]+"))
  segment_languages <- sapply(text_segments, detect_segment_language)
  return(segment_languages)
}

process_word <- function(file_path) {
  doc <- tryCatch({
    read_docx(file_path)
  }, error = function(e) {
    return(NULL)
  })
  
  if (is.null(doc)) {
    return(NULL)
  }
  
  text_from_word <- docx_summary(doc)$text
  combined_text <- paste(text_from_word, collapse = " ")
  text_segments <- unlist(strsplit(combined_text, "[\\n\\.]+"))
  segment_languages <- sapply(text_segments, detect_segment_language)
  return(segment_languages)
}

process_excel_or_csv <- function(file_path) {
  file_extension <- tools::file_ext(file_path)
  
  if (file_extension %in% c("xlsx", "xls")) {
    sheets <- tryCatch({
      excel_sheets(file_path)
    }, error = function(e) {
      return(NULL)
    })
    
    if (is.null(sheets)) {
      return(NULL)
    }
    
    all_data <- lapply(sheets, function(sheet) {
      tryCatch({
        read_excel(file_path, sheet = sheet)
      }, error = function(e) {
        return(NULL)
      })
    })
    
    all_data <- Filter(Negate(is.null), all_data)
    
    if (length(all_data) == 0) {
      return(NULL)
    }
    
    all_texts <- unlist(lapply(all_data, function(df) {
      apply(df, 2, function(column) {
        as.character(column)
      })
    }))
  } else if (file_extension == "csv") {
    all_texts <- tryCatch({
      read_csv(file_path, col_types = cols(.default = "c")) %>% 
        unlist() %>% 
        as.character()
    }, error = function(e) {
      return(NULL)
    })
    
    if (is.null(all_texts)) {
      return(NULL)
    }
  } else {
    return(NULL)
  }
  
  combined_text <- paste(all_texts, collapse = " ")
  text_segments <- unlist(strsplit(combined_text, "[\\n\\.]+"))
  segment_languages <- sapply(text_segments, detect_segment_language)
  return(segment_languages)
}

process_html <- function(file_path) {
  html_text <- tryCatch({
    readtext::readtext(file_path)$text
  }, error = function(e) {
    return(NULL)
  })
  
  if (is.null(html_text)) {
    return(NULL)
  }
  
  text_segments <- unlist(strsplit(html_text, "[\\n\\.]+"))
  segment_languages <- sapply(text_segments, detect_segment_language)
  return(segment_languages)
}

process_zip <- function(zip_path, extract_dir) {
  result <- tryCatch({
    unzip(zip_path, exdir = extract_dir)
    list.files(extract_dir, full.names = TRUE)
  }, warning = function(w) {
    message("Warning during unzip: ", w)
    return(NULL)
  }, error = function(e) {
    message("Error during unzip: ", e)
    return(NULL)
  })
  
  return(result)
}

# Directory containing the folders
directory_path <- ""

# Filter MDL to include only rows where Q_presence is "Yes"
MDL_filtered <- MDL %>% filter(Q_presence == "Yes")

# Get folder names from MDL$ID where Q_presence is "Yes"
folder_names <- MDL_filtered$ID

# Create a data frame to store results
results_df <- data.frame(ID = folder_names, stringsAsFactors = FALSE)

# Process each folder by matching with ID
for (id in results_df$ID) {
  folder_path <- file.path(directory_path, id)
  if (!dir.exists(folder_path)) {
    # If the folder does not exist, fill with "Not Present"
    results_df[results_df$ID == id, paste0("Document ", 1:11)] <- "Not Present"
    next
  }
  
  files <- list.files(folder_path, full.names = TRUE)
  
  if (length(files) == 0) {
    # If the folder exists but is empty, fill with "Not Applicable"
    results_df[results_df$ID == id, paste0("Document ", 1:11)] <- "Not Applicable"
    next
  }
  
  # Initialize a vector to store languages for documents in this folder
  languages <- rep("Not Applicable", 11)
  
  # Check for zip files and process them
  for (i in seq_along(files)) {
    file_path <- files[i]
    file_extension <- tools::file_ext(file_path)
    
    if (file_extension == "zip") {
      extract_dir <- tempfile()
      extracted_files <- process_zip(file_path, extract_dir)
      if (!is.null(extracted_files)) {
        files <- c(files, extracted_files)
        files <- files[files != file_path] # Remove the zip file from the list
      } else {
        # If the zip file couldn't be processed, skip this file
        next
      }
    }
  }
  
  for (i in seq_along(files)) {
    file_path <- files[i]
    file_extension <- tools::file_ext(file_path)
    segment_languages <- NULL
    
    segment_languages <- tryCatch({
      if (file_extension == "pdf") {
        process_pdf(file_path)
      } else if (file_extension == "docx") {
        process_word(file_path)
      } else if (file_extension %in% c("xlsx", "xls", "csv")) {
        process_excel_or_csv(file_path)
      } else if (file_extension == "html") {
        process_html(file_path)
      } else {
        "Unsupported file type"
      }
    }, error = function(e) {
      "Cannot be opened"
    })
    
    # Summarize the most frequent language
    if (!is.null(segment_languages) && length(segment_languages) > 0) {
      # Filter out "Uncertain" and "No content"
      segment_languages <- segment_languages[!segment_languages %in% c("Uncertain", "No content")]
      
      if (length(segment_languages) == 0) {
        languages[i] <- "Unrecognizable"  # No valid languages found
      } else {
        language_summary <- table(segment_languages)
        sorted_languages <- sort(language_summary, decreasing = TRUE)
        
        # Select the most frequent language that is not "Uncertain"
        valid_languages <- names(sorted_languages)
        most_frequent_language <- valid_languages[valid_languages != "Uncertain"][1]
        
        if (is.na(most_frequent_language)) {
          languages[i] <- "Unrecognizable"
        } else {
          languages[i] <- most_frequent_language
        }
      }
    } else {
      languages[i] <- "Cannot be opened"
    }
  }
  
  # Add languages to the results_df
  results_df[results_df$ID == id, paste0("Document ", 1:11)] <- languages
}



```




# For "Unrecognizable"

```{r}


# Convert PDF pages to images and extract text 
convert_pdf_to_images_and_extract_text <- function(pdf_file_path, dpi = 600) {
  # Convert PDF to images with higher DPI for better OCR quality
  images <- pdf_convert(pdf_file_path, dpi = dpi)
  
  
  ocr_engine <- tesseract(language = "eng+ara+fra")
  
  
  extract_text <- function(image_path) {
    text <- ocr(image_path, engine = ocr_engine)
    return(text)
  }
  
 
  texts <- lapply(images, extract_text)
  
  
  full_text <- paste(unlist(texts), collapse = "\n")
  return(full_text)
}


for (id in results_df$ID) {
  folder_path <- file.path(directory_path, id)
  
  files <- list.files(folder_path, full.names = TRUE)
  
  
  for (j in seq_along(files)) {
    document_col <- paste0("Document ", j)
    
    if (results_df[results_df$ID == id, document_col] == "Unrecognizable" && tools::file_ext(files[j]) == "pdf") {
      ocr_text <- convert_pdf_to_images_and_extract_text(files[j], dpi = 600)
      detected_lang <- tryCatch({
        cld2::detect_language(ocr_text, plain_text = TRUE)
      }, error = function(e) {
        return(NA)
      })
      
   
      if (is.na(detected_lang) || detected_lang == "UNKNOWN") {
        results_df[results_df$ID == id, document_col] <- "Unable to detect"
      } else {
        results_df[results_df$ID == id, document_col] <- detected_lang
      }
    }
  }
}





```

